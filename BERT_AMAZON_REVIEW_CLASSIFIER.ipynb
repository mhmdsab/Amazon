{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAZON REVIEW CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to implement a review classification model using Bert model. pretrained Bert models are available\n",
    "on tfhub modules and can also be implemented using Hugging face library.\n",
    "In this notebook we are going to use Hugging face library.\n",
    "\n",
    "The dataset can be downloaded using this link https://www.kaggle.com/bittlingmayer/amazonreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 64\n",
    "truncation_strategy = 'longest_first' #the tokenizer truncates the longer sentence first\n",
    "Tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 4 million review with 90% of them for training and the rest is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./train', 'r', encoding=\"utf8\") as in_file:\n",
    "    train = in_file.readlines()\n",
    "\n",
    "with open('./test', encoding=\"utf8\") as in_file:\n",
    "    test = in_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to prepare the data\n",
    "\n",
    "def split_from_label(line):\n",
    "    return ' '.join(line.split(' ')[1:])\n",
    "\n",
    "def map_label(row):\n",
    "    if row == '__label__1':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the dataset composed of three parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - The label in the form _ _ _label_ _ _  + 1 or 2 for good or bad reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - the head of the review ending with :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - the body of the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes the text file object and returns a data frame with a separate column for each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(text_file):\n",
    "    df = pd.DataFrame(text_file, columns = ['original_text'])\n",
    "    df['head'] = df['original_text'].str.split(':').str.get(0).map(split_from_label).astype('str')\n",
    "    df['body'] = df['original_text'].str.split(':').str.get(1).astype('str')\n",
    "    df['label'] = df['original_text'].str.split(':').str.get(0).str.extract(r'(__label__[1-2])').astype('str')\n",
    "    df['label'] = df['label'].map(map_label)\n",
    "    df.drop('original_text',1,inplace = True)\n",
    "    df = shuffle(df)\n",
    "    return df\n",
    "\n",
    "train_dataset = prepare_dataset(train)\n",
    "test_dataset = prepare_dataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert tokenizer has its own way of tokenization, It takes a sentence and optionally another sentence and converts them into an input suitable for Bert model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of tokenizer encoding method is a dictionary contains the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - the indices of the words of the two sentences separated by [SEP] token and before them the indix of the [CLS] token (in case 'add_special_tokens' argument is set to True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - binary array indicating the non padded part of the sequence needed for attention (1 for non padded and 0 for padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - another binary array indicating which part of the total sequence belongs to sentence 1 and which part belongs to sequence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert text into suitable Bert input form\n",
    "def get_model_inputs(str1, str2, _truncation_strategy, length, tokenizer, pad_seq = True):\n",
    "\n",
    "    inputs = tokenizer.encode_plus(str1,\n",
    "                                    str2,\n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=length,\n",
    "                                    truncation_strategy=_truncation_strategy,\n",
    "                                    pad_to_max_length=pad_seq)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_masks = inputs[\"attention_mask\"]\n",
    "    input_segments = inputs[\"token_type_ids\"]\n",
    "    return [input_ids, input_masks, input_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom data generator to feed the data to the odel batch by batch\n",
    "def datagen(dataset, batch_size):\n",
    "  data = dataset.copy()\n",
    "  while True:\n",
    "    for i in range(1):\n",
    "      inputs_x_id, inputs_x_mask, inputs_x_segment, inputs_y = [], [], [], []\n",
    "      start = i*batch_size\n",
    "      end = start+batch_size\n",
    "      batch_x = data.iloc[start:end,[0, 1]].values\n",
    "      batch_y = data.iloc[start:end,2].values\n",
    "      for i in range(batch_size):\n",
    "        input_ids, input_masks, input_segments = get_model_inputs(batch_x[i,0], batch_x[i,1], \n",
    "                                                                  truncation_strategy, \n",
    "                                                                  MAX_SEQUENCE_LENGTH, \n",
    "                                                                  Tokenizer)\n",
    "        \n",
    "        inputs_x_id.append(input_ids)\n",
    "        inputs_x_mask.append(input_masks)\n",
    "        inputs_x_segment.append(input_segments)\n",
    "        inputs_y.append(batch_y[i])\n",
    "\n",
    "      yield ([np.array(inputs_x_id, dtype=np.int32),\n",
    "             np.array(inputs_x_mask, dtype=np.int32),\n",
    "             np.array(inputs_x_segment, dtype=np.int32)],\n",
    "             np.array(inputs_y, dtype=np.int32))\n",
    "\n",
    "K.clear_session()\n",
    "train_data_generator = datagen(train_dataset, TRAIN_BATCH_SIZE)\n",
    "test_data_generator = datagen(test_dataset, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes the output hidden state of the last layer for all tokens, average pooling them then to a dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can edit the configurations the pass the edited dictionary to the TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    input_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    config = BertConfig() \n",
    "    config.output_hidden_states = False # Set to True to obtain hidden states\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    input_embedding = bert_model(input_id, attention_mask=input_mask, token_type_ids=input_atn)[0]\n",
    "\n",
    "    # Get average tokens output\n",
    "    tokens_embedding = tf.keras.layers.GlobalAveragePooling1D()(input_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(token_embedding)\n",
    "        \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_id, input_mask, input_atn], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "bert = create_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function (modified binary cross entropy loss function which gives higher attention to misclassified examples)\n",
    "def focal_loss(y_true, y_pred, gamma=2., alpha=.25):\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor= 'val_acc', \n",
    "                               mode = 'max',\n",
    "                               patience=30, \n",
    "                               verbose=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('BERT_MODEL_AMAZON_REVIEW_CLASSIFIER',\n",
    "                                   monitor = 'val_acc', \n",
    "                                   mode = 'max', \n",
    "                                   save_best_only=True, \n",
    "                                   verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', \n",
    "                              mode = 'max',\n",
    "                              factor=0.2, \n",
    "                              patience=4, \n",
    "                              min_lr=0.0000001, \n",
    "                              verbose=1)\n",
    "opt = Adam(lr = 0.0005)\n",
    "bert.compile(loss = focal_loss, optimizer= opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "history = bert.fit_generator(generator=train_data_generator,\n",
    "                    validation_data=test_data_generator,\n",
    "                    steps_per_epoch = len(train_dataset)//TRAIN_BATCH_SIZE,\n",
    "                    validation_steps = len(test_dataset)//TEST_BATCH_SIZE,\n",
    "                    epochs = 200,\n",
    "                    callbacks = [early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
